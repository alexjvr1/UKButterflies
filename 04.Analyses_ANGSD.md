# Analyses with ANGSD

Pipeline: 

1. Adapter trimming with cutadapt

2. Map trimmed reads to reference genome using bwa mem

3. MapDamage2 to assess degradation of museum samples and rescale museum data

4. Call variants with ANGSD

5. Downstream analyses


## Install packages on BlueCrystal

#### MapDamage

Install info [here](http://ginolhac.github.io/mapDamage/)

** NB: Installation will only work with Python 2.7.6!

```
module load tools/git-2.22.0

module load languages/python-2.7.6

cd Software

git clone https://github.com/ginolhac/mapDamage.git

python setup.py install --user

PATH="$PATH:$HOME/.local/bin"

cd

mapDamage 
##Should give the following message:
Usage: mapDamage [options] -i BAMfile -r reference.fasta

Use option -h or --help for help

mapDamage: error: SAM/BAM file not given (-i)
```

Check if pysam is installed. This should be there by default
```
python -c "import pysam"

##if nothing happens pysam is installed
```

Check that all the R packages are installed locally: 
```
R

library("inline")
library("gam")
library("Rcpp")
library("ggplot2")
library("RcppGSL")

###I had to install gam and RcppGSL

install.packages("gam")
install.packages("RcpGSL")
```


Now MapDamage should be ready to go!


#### ANGSD

1. Install ANGSD





2. Install [PCAngsd](https://github.com/Rosemeis/pcangsd) locally on blue crystal
```
cd $HOME/Software
module load tools/git-2.22.0

module load languages/python-2.7.6

##.. follow instructions on website
```






## MapDamage

move the [04a_mapDamage_modern.sh](https://github.com/alexjvr1/UKButterflies/blob/master/04a_mapDamage_modern.sh) and [04a_mapDamage_museum.sh](https://github.com/alexjvr1/UKButterflies/blob/master/04a_mapDamage_museum.sh) scripts to the /mapped folder

```
cd SpeciesFolder/mapped/

ls *bam >> bamfiles.names

qsub mapDamage.sh
```

Remember to change the number of threads requested. 

This script will calculate stats for each individual. The museum script will also recalibrate mapping scores for the samples that need it. 

Figures: 
```
##In mapped folder on server: 
for i in $(ls results_*/5pCtoT_freq.txt); do awk '{print $2}' $i >> 5pCtoT.txt; done

#remove header lines from within the file
sed -i 's:5pC>T::g' 5pCtoT.txt
sed -i '/^$/d' 5pCtoT.txt

ls results_* |grep "results_" > sample.names
sed -i 's:results_::g' sample.names
sed -i 's/_cutadapt_filtered_R1.fastq.gz://g' sample.names

################################################
##Import these into R for further manipulation: 
################################################

A1.5pCtoT <- read.table("5pCtoT.txt", header=F)
A1.names <- read.table("sample.names", header=F)

A1.5pCtoT$pos <- rep(1:25, 92) ##add site position. Here 92 is the number of individuals in the full dataset
A1.5pCtoT$Species <- rep("A1.Lymantria_monacha", 2300) ##2300 = 25positions x 92 indivs

A1.names$index <- paste(1:92)
A1.5pCtoT$Sample.names <- A1.names$V1[match(A1.5pCtoT$Sample, A1.names$index)] ##lookup table to add individual names
A1.5pCtoT$pop <- rep(c("A1.mus", "A1.mod.C"), c(1300, 1000) ## check the sample order here before adding these pops
A1.5pCtoT$percentage <- A1.5pCtoT$pop$PropCtoT*100


##This is the current plot. When I have more species I will plot the mean and variance at each position, and show different species with different shapes. I think colour by museum/modern works well

ggplot(A1.5pCtoT.new, aes(x=pos, y=percentage, colour=pop)) + geom_point() + ylab("C to T substitutions (%)") + xlab("Distance from start of read (bp)")
```

![alt_txt][MapDamage_A1]

[MapDamage_A1]:https://user-images.githubusercontent.com/12142475/61050189-465bcf80-a3de-11e9-8832-ea26c4d32868.png





## Downsample data to same depth

Museum samples tend to sequence to lower depth than modern samples due to the deterioration of the DNA fragments. To reduce the bias between the datasets that could be attributed to differences in mean depth, I will downsample the modern data to the same depth as museum within each species. This leaves 3 datasets: Modern.full, Modern.subset, Museum.


Filter for mapping quality and properly paired reads in each sample. Do this for modern and recalibrated museum samples. Make sure to move any museum samples that didn't need recalibration to the recalibrated.mus.bam folder to make up the full data complement. 
```
#!/bin/bash
#PBS -N A1LM.BAMflt_mod1  ##job name
#PBS -l nodes=1:ppn=1  #nr of nodes and processors per node
#PBS -l mem=16gb #RAM
#PBS -l walltime=10:00:00 ##wall time.  
#PBS -j oe  #concatenates error and output files (with prefix job1)
#PBS -t 1-40

#run job in working directory
cd $PBS_O_WORKDIR 

#Load modules
module load apps/samtools-1.8

#Define variables
NAME=$(sed "${PBS_ARRAYID}q;d" mod.bam.names)

echo "samtools filtering started" >> flt.log
echo "---------------" >> flt.log


##Map with BWA MEM and output sorted bam file

sample_name=`echo ${NAME} | awk -F "_cutadapt" '{print $1}'`
echo "[filtering] $sample_name"
printf "\n"
echo "time samtools view -f 2 -q 20 -b $NAME > $NAME.flt.bam" >> flt.log
time samtools view -f 2 -q 20 -b $NAME > $NAME.flt.bam
```

Calculate the number of reads in each sample as a proxy for depth
```
module load apps/samtools-1.8

for i in $(ls *flt.bam); do ls $i >> flagstat.mus.flt.log & samtools flagstat $i >> flagstat.mod.flt.log; done 

grep bam flagstat.mod.flt.log
grep "properly paired" flagstat.mod.flt.log | awk -F 
```

Do the same for the museum samples
```
cat Bam.filter.sh 
#!/bin/bash
#PBS -N A1LM.BAMflt_mus1  ##job name
#PBS -l nodes=1:ppn=1  #nr of nodes and processors per node
#PBS -l mem=16gb #RAM
#PBS -l walltime=10:00:00 ##wall time.  
#PBS -j oe  #concatenates error and output files (with prefix job1)
#PBS -t 1-52

#run job in working directory
cd $PBS_O_WORKDIR 

#Load modules
module load apps/samtools-1.8

#Define variables
NAME=$(sed "${PBS_ARRAYID}q;d" not.rescaled.mus.samples.names)

echo "samtools filtering started" >> flt.log
echo "---------------" >> flt.log


##Map with BWA MEM and output sorted bam file

sample_name=`echo ${NAME} | awk -F "_cutadapt" '{print $1}'`
echo "[filtering] $sample_name"
printf "\n"
echo "time samtools view -f 2 -q 20 -b $NAME > $NAME.flt.bam" >> flt.log
time samtools view -f 2 -q 20 -b $NAME > $NAME.flt.bam

```

Calculate the number of reads in each sample as a proxy for depth
```
module load apps/samtools-1.8

for i in $(ls *flt.bam); do ls $i >> flagstat.mus.flt.log & samtools flagstat $i >> flagstat.mus.flt.log; done 

grep bam flagstat.mus.flt.log
grep "properly paired" flagstat.mus.flt.log | awk -F 
```
Use this data to update the spreadsheet in dropbox (Velocity_MappingStatsPerSpecies_AJvR_20190604.xlsx).





Downsample modern dataset using samtools. [Downsample.sh](https://github.com/alexjvr1/UKButterflies/blob/master/Downsample.sh)

These bam files can now be used for ANGSD analyses: 

Modern: Downsampled bam files and filtered for mapping quality >20 and properly paired reads only

Museum: recalibrated based on MapDamage2 output, and filtered for mapping quality >20 and properly paired reads only. 


## Analyses

wd: 
/newhome/aj18951/G3_Hesperia_comma/ANGSD

## 1. Test the best Genotype Likelihood model to use

I will test this with G1, G2, and G3 following the example on [ANGSD](http://www.popgen.dk/angsd/index.php/Glcomparison)

Where 1=Samtools, 2=GATK (original model), 3=SOAPsnp, 4=SYK

ANGSD parallelises analyses, but not file reading (-P specifies the number of threads). See [here](https://github.com/ANGSD/angsd/issues/74) for comments on the issue. The most efficient solution is to split the regions file (-rf) - similar to the approach used to speed up variant calling with samtools/bcftools. 

I'll use the same regions files, but they need to be modified for ANGSD input. 

```
cp ~/G3_Hesperia_comma/03_variants/regions* ~/G3_Hesperia_comma/ANGSD/

for i in $(ls regions); do sed -i 's/,/\n/g' $i; done
```

For the initial tests I will use just a small subset of regions to minimise computing time. For G3 each regions file contains 5200 regions and the last file (regionsbe) contains 300. I'll split regionsbe into 3 files and run scripts in parallel. For the final script I'll run arrays of 100 regions in each job 

```
split -l 100 regionsbe regions100
```

And make three ANGSD input files
```
~/software/angsd/angsd -bam ~/G3_Hesperia_comma/mapped.modern.core.names -minInd 18 -minQ 20 -minMapQ 20 -uniqueOnly 1 -remove_bads 1 -only_proper_pairs 1 -GL 1 -doSaf 1 -out G3.GL1.test -rf regions100aa -anc ../RefGenome/Hesperia_comma

```







Following [this tutorial](https://github.com/mfumagalli/ngsTools/blob/master/TUTORIAL.md) by Fumagalli. 



I've downloaded ANGSD into by bin folder and added to PATH
```
PATH="$PATH:~bin/angsd/"

```

Download NGSTools to bin folder and add to PATH
```
module load tools/git-2.18.0
module load languages/gcc-5.0
module load tools/zlib-1.2.8

module load languages/R-3.0.2
module load languages/perl-5.14.2


```


Calculate Fst and PBS ([Population Branch statistic](https://www.biostars.org/p/297337/))


